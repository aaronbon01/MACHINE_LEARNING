{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aaronbon01/MACHINE_LEARNING/blob/main/5_Regresi%C3%B3n%20Log%C3%ADstica%20-%20Detecci%C3%B3n%20de%20SPAM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aLt2OX1FqKTa"
      },
      "source": [
        "# Regresión Logística: Detección de SPAM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zO5hlDymqKTc"
      },
      "source": [
        "En este ejercicio se muestran los fundamentos de la Regresión Logística planteando uno de los primeros problemas que fueron solucionados mediante el uso de técnicas de Machine Learning: la detección de SPAM."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0Tkkup7qKTc"
      },
      "source": [
        "## Enunciado del ejercicio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eIms5RO1qKTc"
      },
      "source": [
        "Se propone la construcción de un sistema de aprendizaje automático capaz de predecir si un correo determinado se corresponde con un correo de SPAM o no, para ello, se utilizará el siguiente conjunto de datos:\n",
        "\n",
        "##### [2007 TREC Public Spam Corpus](https://plg.uwaterloo.ca/cgi-bin/cgiwrap/gvcormac/foo07)\n",
        "The corpus trec07p contains 75,419 messages:\n",
        "\n",
        "    25220 ham\n",
        "    50199 spam\n",
        "\n",
        "These messages constitute all the messages delivered to a particular\n",
        "server between these dates:\n",
        "\n",
        "    Sun, 8 Apr 2007 13:07:21 -0400\n",
        "    Fri, 6 Jul 2007 07:04:53 -0400"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-rW1pQvqKTd"
      },
      "source": [
        "### 1. Funciones complementarias"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3PiNioUUqKTd"
      },
      "source": [
        "En este caso práctico relacionado con la detección de correos electrónicos de SPAM, el conjunto de datos que disponemos esta formado por correos electrónicos, con sus correspondientes cabeceras y campos adicionales. Por lo tanto, requieren un preprocesamiento previo a que sean ingeridos por el algoritmo de Machine Learning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "zTAqMN-oqKTd"
      },
      "outputs": [],
      "source": [
        "# Esta clase facilita el preprocesamiento de correos electrónicos que poseen código HTML\n",
        "from html.parser import HTMLParser\n",
        "\n",
        "class MLStripper(HTMLParser):\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "        self.strict = False\n",
        "        self.convert_charrefs = True\n",
        "        self.fed = []\n",
        "\n",
        "    def handle_data(self, d):\n",
        "        self.fed.append(d)\n",
        "\n",
        "    def get_data(self):\n",
        "        return ''.join(self.fed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "2lzOzSfAqKTe"
      },
      "outputs": [],
      "source": [
        "# Esta función se encarga de elimar los tags HTML que se encuentren en el texto del correo electrónico\n",
        "def strip_tags(html):\n",
        "    s = MLStripper()\n",
        "    s.feed(html)\n",
        "    return s.get_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "5iVr4UwYqKTe",
        "outputId": "927b68b1-f61d-4bc0-dfb5-a137e4d72b48"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Phrack World News'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "# Ejemplo de eliminación de los tags HTML de un texto\n",
        "t = '<tr><td align=\"left\"><a href=\"../../issues/51/16.html#article\">Phrack World News</a></td>'\n",
        "strip_tags(t)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oU8Xq-eiqKTf"
      },
      "source": [
        "Además de eliminar los posibles tags HTML que se encuentren en el correo electrónico, deben realizarse otras acciones de preprocesamiento para evitar que los mensajes contengan ruido innecesario. Entre ellas se encuentra la eliminación de los signos de puntuación, eliminación de posibles campos del correo electrónico que no son relevantes o eliminación de los afijos de una palabra manteniendo únicamente la raiz de la misma (Stemming). La clase que se muestra a continuación realiza estas transformaciones."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "hX2eJ_twqKTf"
      },
      "outputs": [],
      "source": [
        "import email\n",
        "import string\n",
        "import nltk\n",
        "\n",
        "class Parser:\n",
        "\n",
        "    def __init__(self):\n",
        "        self.stemmer = nltk.PorterStemmer()\n",
        "        self.stopwords = set(nltk.corpus.stopwords.words('english'))\n",
        "        self.punctuation = list(string.punctuation)\n",
        "\n",
        "    def parse(self, email_path):\n",
        "        \"\"\"Parse an email.\"\"\"\n",
        "        with open(email_path, errors='ignore') as e:\n",
        "            msg = email.message_from_file(e)\n",
        "        return None if not msg else self.get_email_content(msg)\n",
        "\n",
        "    def get_email_content(self, msg):\n",
        "        \"\"\"Extract the email content.\"\"\"\n",
        "        subject = self.tokenize(msg['Subject']) if msg['Subject'] else [] #self - llamo a una funcion del propio proceso\n",
        "        body = self.get_email_body(msg.get_payload(),\n",
        "                                   msg.get_content_type())\n",
        "        content_type = msg.get_content_type()\n",
        "        # Returning the content of the email\n",
        "        return {\"subject\": subject,\n",
        "                \"body\": body,\n",
        "                \"content_type\": content_type}\n",
        "\n",
        "    def get_email_body(self, payload, content_type):\n",
        "        \"\"\"Extract the body of the email.\"\"\"\n",
        "        body = []\n",
        "        if type(payload) is str and content_type == 'text/plain':\n",
        "            return self.tokenize(payload)\n",
        "        elif type(payload) is str and content_type == 'text/html':\n",
        "            return self.tokenize(strip_tags(payload))\n",
        "        elif type(payload) is list:\n",
        "            for p in payload:\n",
        "                body += self.get_email_body(p.get_payload(),\n",
        "                                            p.get_content_type())\n",
        "        return body\n",
        "\n",
        "    def tokenize(self, text):\n",
        "        \"\"\"Transform a text string in tokens. Perform two main actions,\n",
        "        clean the punctuation symbols and do stemming of the text.\"\"\"\n",
        "        for c in self.punctuation:\n",
        "            text = text.replace(c, \"\")\n",
        "        text = text.replace(\"\\t\", \" \") #eliminar tabulador\n",
        "        text = text.replace(\"\\n\", \" \") #eliminar salto de línea\n",
        "        tokens = list(filter(None, text.split(\" \")))\n",
        "        # Stemming of the tokens\n",
        "        return [self.stemmer.stem(w) for w in tokens if w not in self.stopwords]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Que se hace en [self.stemmer.stem(w) for w in tokens if w not in self.stopwords]\n",
        "#l=[]\n",
        "#for w in tokens\n",
        "#  if w not in self.stopwords\n",
        "#    l.append(stemmer.stem(w))\n"
      ],
      "metadata": {
        "id": "nyO1PYUOxAVc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A3QnqInTqKTf"
      },
      "source": [
        "##### Lectura de un correo en formato raw"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 183
        },
        "id": "mPQKLziAqKTg",
        "outputId": "54a1ed89-4f0d-4f01-e993-13c9c37c3a03"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-8f089a616af6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0minmail\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"datasets/trec07p/data/inmail.1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minmail\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'datasets/trec07p/data/inmail.1'"
          ]
        }
      ],
      "source": [
        "inmail = open(\"datasets/trec07p/data/inmail.1\").read()\n",
        "print(inmail)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7PznJGDWqKTg"
      },
      "source": [
        "##### Parsing del correo electrónico"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "TRZzjHyzqKTg",
        "outputId": "3049924f-52db-47a9-e34f-d9f91b084e6e"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "LookupError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     83\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m                     \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{self.subdir}/{zip_name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 583\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords.zip/stopwords/\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-5182564cabfd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mParser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"datasets/trec07p/data/inmail.1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-f1324a9b6799>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstemmer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPorterStemmer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstopwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpunctuation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpunctuation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"LazyCorpusLoader object has no attribute '__bases__'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m         \u001b[0;31m# This looks circular, but its not, since __load() changes our\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;31m# __class__ to something new:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     84\u001b[0m                     \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{self.subdir}/{zip_name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0;31m# Load the corpus.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m                 \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{self.subdir}/{self.__name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    581\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"*\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 583\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
          ]
        }
      ],
      "source": [
        "p = Parser()\n",
        "p.parse(\"datasets/trec07p/data/inmail.1\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hj26jSIjqKTg"
      },
      "source": [
        "##### Lectura del índice"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NIDkdwzdqKTg"
      },
      "source": [
        "Estas funciones complementarias se encargan cargar en memoria la ruta de cada correo electrónico y su etiqueta correspondiente {spam, ham}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FlvHHcnKqKTg"
      },
      "outputs": [],
      "source": [
        "index = open(\"datasets/trec07p/full/index\").readlines()\n",
        "index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3zh0aDZ_qKTh"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "DATASET_PATH = \"datasets/trec07p\"\n",
        "\n",
        "def parse_index(path_to_index, n_elements):\n",
        "    ret_indexes = []\n",
        "    index = open(path_to_index).readlines()\n",
        "    for i in range(n_elements):\n",
        "        mail = index[i].split(\" ../\")\n",
        "        label = mail[0]\n",
        "        path = mail[1][:-1]\n",
        "        ret_indexes.append({\"label\":label, \"email_path\":os.path.join(DATASET_PATH, path)})\n",
        "    return ret_indexes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a1qfOAJkqKTh"
      },
      "outputs": [],
      "source": [
        "temail = 'ham ../data/inmail.1000\\n'.split(\" ../\")\n",
        "print(temail)\n",
        "print(temail[0])\n",
        "print(temail[1][:-1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eexcr12CqKTh"
      },
      "outputs": [],
      "source": [
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ENQlfNQHqKTh"
      },
      "outputs": [],
      "source": [
        "{\"label\":temail[0], \"email_path\":os.path.join(\"datasets/trec07p\", temail[1][:-1])}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "26vhMwC7qKTh"
      },
      "outputs": [],
      "source": [
        "def parse_email(index):\n",
        "    p = Parser()\n",
        "    pmail = p.parse(index[\"email_path\"])\n",
        "    return pmail, index[\"label\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "09UpvlKwqKTh"
      },
      "outputs": [],
      "source": [
        "indexes = parse_index(\"datasets/trec07p/full/index\", 10)\n",
        "indexes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yz8UvEkwqKTi"
      },
      "source": [
        "### 2. Preprocesamiento de los datos del conjunto de datos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UBwE37r5qKTi"
      },
      "source": [
        "Con las funciones presentadas anteriormente se permite la lectura de los correos electrónicos de manera programática y el procesamiento de los mismos para eliminar aquellos componentes que no resultan de utilidad para la detección de correos de SPAM. Sin embargo, cada uno de los correos sigue estando representado por un diccionario de Python con una serie de palabras."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ymXlGyHgqKTi"
      },
      "outputs": [],
      "source": [
        "# Cargamos el índice y las etiquetas en memoria\n",
        "index = parse_index(\"datasets/trec07p/full/index\", 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JJrkQnRJqKTi"
      },
      "outputs": [],
      "source": [
        "# Leemos el primer correo\n",
        "import os\n",
        "\n",
        "open(index[0][\"email_path\"]).read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k_gscZljqKTi"
      },
      "outputs": [],
      "source": [
        "# Parseamos el primer correo\n",
        "mail, label = parse_email(index[0])\n",
        "print(\"El correo es:\", label)\n",
        "print(mail)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8LIG2qBqKTi"
      },
      "source": [
        "El algoritmo de Regresión Logística no es capaz de ingerir texto como parte del conjunto de datos. Por lo tanto, deben aplicarse una serie de funciones adicionales que transformen el texto de los correos electrónicos parseados en una representación numérica."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_rZo8ZyrqKTj"
      },
      "source": [
        "##### Aplicación de CountVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uED607APqKTj"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Preapración del email en una cadena de texto\n",
        "prep_email = [\" \".join(mail['subject']) + \" \".join(mail['body'])]\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit(prep_email)\n",
        "\n",
        "print(\"Email:\", prep_email, \"\\n\")\n",
        "print(\"Características de entrada:\", vectorizer.get_feature_names())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xy2oZD-NqKTj"
      },
      "outputs": [],
      "source": [
        "X = vectorizer.transform(prep_email)\n",
        "print(\"\\nValues:\\n\", X.toarray())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rxYhtEJtqKTj"
      },
      "source": [
        "##### Aplicación de OneHotEncoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TVlQQgXYqKTj"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "prep_email = [[w] for w in mail['subject'] + mail['body']]\n",
        "\n",
        "enc = OneHotEncoder(handle_unknown='ignore')\n",
        "X = enc.fit_transform(prep_email)\n",
        "\n",
        "print(\"Features:\\n\", enc.get_feature_names())\n",
        "print(\"\\nValues:\\n\", X.toarray())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_FvNFGbOqKTj"
      },
      "source": [
        "##### Funciones auxiliares para preprocesamiento del conjunto de datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Csaf9ubqKTj"
      },
      "outputs": [],
      "source": [
        "def create_prep_dataset(index_path, n_elements):\n",
        "    X = []\n",
        "    y = []\n",
        "    indexes = parse_index(index_path, n_elements)\n",
        "    for i in range(n_elements):\n",
        "        print(\"\\rParsing email: {0}\".format(i+1), end='')\n",
        "        mail, label = parse_email(indexes[i])\n",
        "        X.append(\" \".join(mail['subject']) + \" \".join(mail['body']))\n",
        "        y.append(label)\n",
        "    return X, y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wdbah3iTqKTk"
      },
      "source": [
        "### 3. Entrenamiento del algoritmo "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k34yHSEhqKTk"
      },
      "outputs": [],
      "source": [
        "# Leemos únicamente un subconjunto de 100 correos electrónicos\n",
        "X_train, y_train = create_prep_dataset(\"datasets/trec07p/full/index\", 100)\n",
        "X_train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6rmnOpxJqKTk"
      },
      "source": [
        "##### Aplicamos la vectorización a los datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J3jPhzQiqKTk"
      },
      "outputs": [],
      "source": [
        "vectorizer = CountVectorizer()\n",
        "X_train = vectorizer.fit_transform(X_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H8xYZqKuqKTk"
      },
      "outputs": [],
      "source": [
        "print(X_train.toarray())\n",
        "print(\"\\nFeatures:\", len(vectorizer.get_feature_names()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tSs5VNQTqKTk"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "pd.DataFrame(X_train.toarray(), columns=[vectorizer.get_feature_names()])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vyL3SBz-qKTk"
      },
      "outputs": [],
      "source": [
        "y_train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QdMxHE9xqKTl"
      },
      "source": [
        "###### Entrenamiento del algoritmo de regresión logística con el conjunto de datos preprocesado"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6m_t09WyqKTl"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "clf = LogisticRegression()\n",
        "clf.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LekaJLr4qKTl"
      },
      "source": [
        "### 4. Predicción"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gNE-xoNVqKTl"
      },
      "source": [
        "##### Lectura de un conjunto de correos nuevos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D5sijsgAqKTl"
      },
      "outputs": [],
      "source": [
        "# Leemos 150 correos de nuestro conjunto de datos y nos quedamos únicamente con los 50 últimos \n",
        "# Estos 50 correos electrónicos no se han utilizado para entrenar el algoritmo\n",
        "X, y = create_prep_dataset(\"datasets/trec07p/full/index\", 150)\n",
        "X_test = X[100:]\n",
        "y_test = y[100:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3YqNPFYqKTl"
      },
      "source": [
        "##### Preprocesamiento de los correos con el vectorizador creado anteriormente"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BOqtuu7kqKTl"
      },
      "outputs": [],
      "source": [
        "X_test = vectorizer.transform(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AxWRZ1NzqKTm"
      },
      "source": [
        "##### Predicción del tipo de correo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dIAsKeYvqKTm"
      },
      "outputs": [],
      "source": [
        "y_pred = clf.predict(X_test)\n",
        "y_pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ESSVh6kiqKTm"
      },
      "outputs": [],
      "source": [
        "print(\"Predicción:\\n\", y_pred)\n",
        "print(\"\\nEtiquetas reales:\\n\", y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ef6626d1qKTm"
      },
      "source": [
        "##### Evaluación de los resultados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Va9mfSPSqKTm"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "print('Accuracy: {:.3f}'.format(accuracy_score(y_test, y_pred)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bWXCiTrqqKTm"
      },
      "source": [
        "### 5. Aumentando el conjunto de datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qV67oz6BqKTm"
      },
      "outputs": [],
      "source": [
        "# Leemos 12000 correos electrónicos\n",
        "X, y = create_prep_dataset(\"datasets/trec07p/full/index\", 12000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ez2sAe1yqKTm"
      },
      "outputs": [],
      "source": [
        "# Utilizamos 10000 correos electrónicos para entrenar el algoritmo y 2000 para realizar pruebas\n",
        "X_train, y_train = X[:10000], y[:10000]\n",
        "X_test, y_test = X[10000:], y[10000:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "brukhGFBqKTn"
      },
      "outputs": [],
      "source": [
        "vectorizer = CountVectorizer()\n",
        "X_train = vectorizer.fit_transform(X_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MoGKk1VqqKTn"
      },
      "outputs": [],
      "source": [
        "clf = LogisticRegression()\n",
        "clf.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TVPl_2tHqKTn"
      },
      "outputs": [],
      "source": [
        "X_test = vectorizer.transform(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ML7AZv49qKTn"
      },
      "outputs": [],
      "source": [
        "y_pred = clf.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rEejYyQhqKTn"
      },
      "outputs": [],
      "source": [
        "print('Accuracy: {:.3f}'.format(accuracy_score(y_test, y_pred)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lmWwXkSJqKTn"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}